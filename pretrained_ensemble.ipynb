{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "data_dir = 'path/to/oxford_iiit_pet_dataset'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df['image'][0]\n",
    "\n",
    "\n",
    "\n",
    "data = df.copy(deep=True)\n",
    "\n",
    "# preprocess image data\n",
    "# data['image'] = data['image'].apply(lambda x: np.array(Image.fromarray(x).resize((224, 224))))\n",
    "# data['image'] = data['image'].apply(lambda x: x / 255.0)\n",
    "\n",
    "# one-hot encode label\n",
    "# data['label'] = data['label'].apply(lambda x: 1 if x == 'dog' else 0)\n",
    "data = pd.get_dummies(data, columns=['label'])\n",
    "# one-hot encode breed\n",
    "data = pd.get_dummies(data, columns=['breed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# split train into train and validation\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data/images/ to the filenames\n",
    "train['filename'] = 'data/images/' + train['filename']\n",
    "val['filename'] = 'data/images/' + val['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label_0 and label_1 to label_cat and label_dog\n",
    "train = train.rename(columns={'label_0': 'label_cat', 'label_1': 'label_dog'})\n",
    "val = val.rename(columns={'label_0': 'label_cat', 'label_1': 'label_dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train,\n",
    "    x_col='filename',\n",
    "    y_col=['label_cat', 'label_dog'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val,\n",
    "    x_col='filename',\n",
    "    y_col=['label_cat', 'label_dog'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    tf.keras.applications.VGG16(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    tf.keras.applications.ResNet50(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    tf.keras.applications.InceptionV3(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "]\n",
    "\n",
    "# Step 3: Fine-tune the models\n",
    "for model in base_models:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.summary()\n",
    "    x = model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    predictions = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=model.input, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "    model.fit(train_generator, epochs=4, validation_data=val_generator)\n",
    "\n",
    "# Step 4: Combine the predictions\n",
    "test_images = []\n",
    "for images, labels in test_ds:\n",
    "    test_images.append(images.numpy())\n",
    "test_images = np.concatenate(test_images)\n",
    "preds = []\n",
    "for model in base_models:\n",
    "    preds.append(model.predict(test_images))\n",
    "preds = np.concatenate(preds, axis=1)\n",
    "ensemble_preds = np.argmax(np.mean(preds, axis=1), axis=1)\n",
    "\n",
    "# Step 5: Evaluate the ensemble model\n",
    "ensemble_acc = np.mean(ensemble_preds == test_ds.labels)\n",
    "print('Ensemble accuracy:', ensemble_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model):\n",
    "    base_model.trainable = True\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')(global_average_layer)\n",
    "    model = tf.keras.models.Model(inputs=base_model.input, outputs=prediction_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "def fit_model(model):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                      steps_per_epoch=len(total_train)//batch_size, \n",
    "                        epochs=epochs, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        validation_steps=len(total_val)//batch_size)\n",
    "    return history\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    "\n",
    "base_model1 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\")\n",
    "base_model2 = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\")\n",
    "base_model3 = tf.keras.applications.Xception(input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "model1 = create_model(base_model1)\n",
    "model2 = create_model(base_model2)\n",
    "model3 = create_model(base_model3)\n",
    "\n",
    "history1 = fit_model(model1)\n",
    "model1.save('models/model1.h5')\n",
    "history2 = fit_model(model2)\n",
    "model2.save('models/model2.h5')\n",
    "history3 = fit_model(model3)\n",
    "model3.save('models/model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models():\n",
    "    all_models = []\n",
    "    model_names = ['model1.h5', 'model2.h5', 'model3.h5']\n",
    "    for model_name in model_names:\n",
    "        filename = os.path.join('models', model_name)\n",
    "        model = tf.keras.models.load_model(filename)\n",
    "        all_models.append(model)\n",
    "        print('loaded:', filename)\n",
    "    return all_models\n",
    "    \n",
    "models = load_all_models()\n",
    "for i, model in enumerate(models):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_visible = [model.input for model in models]\n",
    "ensemble_outputs = [model.output for model in models]\n",
    "merge = tf.keras.layers.concatenate(ensemble_outputs)\n",
    "merge = tf.keras.layers.Dense(10, activation='relu')(merge)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(merge)\n",
    "model = tf.keras.models.Model(inputs=ensemble_visible, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [X_train for _ in range(len(model.input))]\n",
    "X_test = [X_test for _ in range(len(model.input))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    steps_per_epoch=len(total_train) // batch_size,\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_1, y_test),\n",
    "                    validation_steps=len(total_val) // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
